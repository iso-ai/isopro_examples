{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iso-ai/isopro_examples/blob/main/examples/run_cartpole_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFAjeHqBHD5S"
      },
      "source": [
        "# LLM-based CartPole Reinforcement Learning Agent\n",
        "\n",
        "This notebook demonstrates how to create and train a Reinforcement Learning agent that uses a Large Language Model (LLM) to make decisions in the CartPole environment.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, let's import the necessary libraries and set up our environment."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install isopro"
      ],
      "metadata": {
        "id": "3r6AZrBpHHpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install iso-adverse"
      ],
      "metadata": {
        "id": "BIhfy2K3HtsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iqts6lzHD5T"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from isopro.rl.rl_agent import RLAgent\n",
        "from isopro.rl.rl_environment import LLMRLEnvironment\n",
        "from stable_baselines3 import PPO\n",
        "import numpy as np\n",
        "import anthropic\n",
        "import logging\n",
        "from typing import Optional, Dict, Any\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from datetime import datetime\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Create a folder to store the results\n",
        "output_folder = \"results\"\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye3saQlvHD5U"
      },
      "source": [
        "## Define the LLMCartPoleWrapper\n",
        "\n",
        "Now, let's define our `LLMCartPoleWrapper` class, which will integrate the LLM with the CartPole environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8hde_1dHD5U"
      },
      "outputs": [],
      "source": [
        "class LLMCartPoleWrapper(LLMRLEnvironment):\n",
        "    def __init__(self, agent_prompt, llm_call_limit: int):\n",
        "        super().__init__(agent_prompt, None)\n",
        "        self.cartpole_env = gym.make('CartPole-v1')\n",
        "        self.action_space = self.cartpole_env.action_space\n",
        "        self.observation_space = self.cartpole_env.observation_space\n",
        "        self.client = anthropic.Anthropic(api_key=userdata.get(\"ANTHROPIC_API_KEY\"))\n",
        "        self.llm_call_count = 0\n",
        "        self.llm_call_limit = llm_call_limit  # Set the maximum number of LLM calls allowed\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        # Reset the environment and the LLM call count\n",
        "        self.llm_call_count = 0\n",
        "        return self.cartpole_env.reset(**kwargs)\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.llm_call_count >= self.llm_call_limit:\n",
        "            # If the LLM call limit is reached, take a default action (e.g., action = 0)\n",
        "            logging.warning(\"LLM call limit reached, default action taken\")\n",
        "            return self.cartpole_env.step(0)  # Default action can be customized\n",
        "\n",
        "        # Otherwise, proceed with the LLM call and increment the counter\n",
        "        self.llm_call_count += 1\n",
        "        return self.cartpole_env.step(action)\n",
        "\n",
        "\n",
        "    def _llm_decision_to_cartpole_action(self, llm_decision):\n",
        "        if isinstance(llm_decision, (int, np.integer)):\n",
        "            return llm_decision\n",
        "        elif isinstance(llm_decision, str):\n",
        "            return 0 if \"left\" in llm_decision.lower() else 1\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected action type: {type(llm_decision)}\")\n",
        "\n",
        "    def _update_llm(self, observation, reward, done):\n",
        "        user_message = f\"Observation: {observation}, Reward: {reward}, Done: {done}. What action should we take next?\"\n",
        "\n",
        "        messages = self.conversation_history + [\n",
        "            {\"role\": \"user\", \"content\": user_message},\n",
        "        ]\n",
        "\n",
        "        response = self.client.messages.create(\n",
        "            model=\"claude-3-opus-20240229\",\n",
        "            max_tokens=150,\n",
        "            system=self.agent_prompt,\n",
        "            messages=messages\n",
        "        )\n",
        "\n",
        "        ai_response = response.content[0].text\n",
        "        self.conversation_history.append({\"role\": \"user\", \"content\": user_message})\n",
        "        self.conversation_history.append({\"role\": \"assistant\", \"content\": ai_response})\n",
        "        logger.debug(f\"LLM updated. AI response: {ai_response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdwQjfpTHD5V"
      },
      "source": [
        "## Create and Train the RL Agent\n",
        "\n",
        "Now, let's create our RL agent and train it using the LLM-based CartPole environment.\n",
        "\n",
        "The maximum call limit is set to 100 and total_timesteps to 20 to restrict the amount of LLM calls during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0DJ9vwtHD5V"
      },
      "outputs": [],
      "source": [
        "agent_prompt = \"\"\"You are an AI trained to play the CartPole game.\n",
        "Your goal is to balance a pole on a moving cart for as long as possible.\n",
        "You will receive observations about the cart's position, velocity, pole angle, and angular velocity.\n",
        "Based on these, you should decide whether to move the cart left or right.\n",
        "Respond with 'Move left' or 'Move right' for each decision.\"\"\"\n",
        "\n",
        "env = LLMCartPoleWrapper(agent_prompt, llm_call_limit=100)\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "\n",
        "logger.info(\"Starting training\")\n",
        "model.learn(total_timesteps=20)\n",
        "logger.info(\"Training completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACcF38y8HD5V"
      },
      "source": [
        "## Test the Trained Agent\n",
        "\n",
        "Now that we've trained our agent, let's test it for 2 episodes and see how it performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPB1kJVmHD5V"
      },
      "outputs": [],
      "source": [
        "test_episodes = 2\n",
        "results = []\n",
        "\n",
        "logger.info(\"Starting test episodes\")\n",
        "for episode in tqdm(range(test_episodes), desc=\"Test Episodes\"):\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    episode_length = 0\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, reward, terminated, truncated, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        episode_length += 1\n",
        "        done = terminated or truncated\n",
        "\n",
        "    logger.info(f\"Episode {episode + 1} completed. Total reward: {total_reward}, Length: {episode_length}\")\n",
        "    results.append({\"episode\": episode + 1, \"total_reward\": total_reward, \"length\": episode_length})\n",
        "\n",
        "# Save results to file\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "output_file = os.path.join(output_folder, f\"cartpole_results_{timestamp}.json\")\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "logger.info(f\"Results saved to {output_file}\")\n",
        "\n",
        "# Print summary\n",
        "average_reward = sum(r['total_reward'] for r in results) / len(results)\n",
        "average_length = sum(r['length'] for r in results) / len(results)\n",
        "logger.info(f\"Test completed. Average reward: {average_reward:.2f}, Average length: {average_length:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk3tmO8hHD5W"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we've demonstrated how to:\n",
        "\n",
        "1. Set up an LLM-based wrapper for the CartPole environment\n",
        "2. Train a reinforcement learning agent using this environment\n",
        "3. Test the trained agent and collect performance metrics\n",
        "\n",
        "This approach combines the decision-making capabilities of a large language model with the learning process of reinforcement learning, potentially leading to interesting and novel solutions to the CartPole problem.\n",
        "\n",
        "Feel free to experiment with different prompts, training parameters, or even different environments to see how this approach can be applied in various scenarios!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}