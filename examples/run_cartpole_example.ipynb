{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-based CartPole Reinforcement Learning Agent\n",
    "\n",
    "This notebook demonstrates how to create and train a Reinforcement Learning agent that uses a Large Language Model (LLM) to make decisions in the CartPole environment.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from isopro.rl.rl_agent import RLAgent\n",
    "from isopro.rl.rl_environment import LLMRLEnvironment\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "import anthropic\n",
    "import os\n",
    "import logging\n",
    "from typing import Optional, Dict, Any\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the LLMCartPoleWrapper\n",
    "\n",
    "Now, let's define our `LLMCartPoleWrapper` class, which will integrate the LLM with the CartPole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMCartPoleWrapper(LLMRLEnvironment):\n",
    "    def __init__(self, agent_prompt):\n",
    "        super().__init__(agent_prompt, None)\n",
    "        self.cartpole_env = gym.make('CartPole-v1')\n",
    "        self.action_space = self.cartpole_env.action_space\n",
    "        self.observation_space = self.cartpole_env.observation_space\n",
    "        self.client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "        self.agent_prompt = agent_prompt\n",
    "        logger.info(\"LLMCartPoleWrapper initialized\")\n",
    "\n",
    "    def reset(self, *, seed: Optional[int] = None, options: Optional[Dict[str, Any]] = None):\n",
    "        self.conversation_history = []\n",
    "        obs, info = self.cartpole_env.reset(seed=seed, options=options)\n",
    "        logger.debug(f\"Environment reset. Initial observation: {obs}\")\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        cartpole_action = self._llm_decision_to_cartpole_action(action)\n",
    "        observation, reward, terminated, truncated, info = self.cartpole_env.step(cartpole_action)\n",
    "        self._update_llm(observation, reward, terminated or truncated)\n",
    "        logger.debug(f\"Step taken. Action: {cartpole_action}, Reward: {reward}, Done: {terminated or truncated}\")\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def _llm_decision_to_cartpole_action(self, llm_decision):\n",
    "        if isinstance(llm_decision, (int, np.integer)):\n",
    "            return llm_decision\n",
    "        elif isinstance(llm_decision, str):\n",
    "            return 0 if \"left\" in llm_decision.lower() else 1\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected action type: {type(llm_decision)}\")\n",
    "\n",
    "    def _update_llm(self, observation, reward, done):\n",
    "        user_message = f\"Observation: {observation}, Reward: {reward}, Done: {done}. What action should we take next?\"\n",
    "        \n",
    "        messages = self.conversation_history + [\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "        ]\n",
    "\n",
    "        response = self.client.messages.create(\n",
    "            model=\"claude-3-opus-20240229\",\n",
    "            max_tokens=150,\n",
    "            system=self.agent_prompt,\n",
    "            messages=messages\n",
    "        )\n",
    "\n",
    "        ai_response = response.content[0].text\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "        logger.debug(f\"LLM updated. AI response: {ai_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Train the RL Agent\n",
    "\n",
    "Now, let's create our RL agent and train it using the LLM-based CartPole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_prompt = \"\"\"You are an AI trained to play the CartPole game. \n",
    "Your goal is to balance a pole on a moving cart for as long as possible. \n",
    "You will receive observations about the cart's position, velocity, pole angle, and angular velocity. \n",
    "Based on these, you should decide whether to move the cart left or right. \n",
    "Respond with 'Move left' or 'Move right' for each decision.\"\"\"\n",
    "\n",
    "env = LLMCartPoleWrapper(agent_prompt)\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "logger.info(\"Starting training\")\n",
    "model.learn(total_timesteps=10000)\n",
    "logger.info(\"Training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Trained Agent\n",
    "\n",
    "Now that we've trained our agent, let's test it for 2 episodes and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_episodes = 2\n",
    "results = []\n",
    "\n",
    "logger.info(\"Starting test episodes\")\n",
    "for episode in tqdm(range(test_episodes), desc=\"Test Episodes\"):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    episode_length = 0\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        episode_length += 1\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    logger.info(f\"Episode {episode + 1} completed. Total reward: {total_reward}, Length: {episode_length}\")\n",
    "    results.append({\"episode\": episode + 1, \"total_reward\": total_reward, \"length\": episode_length})\n",
    "\n",
    "# Save results to file\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = os.path.join(output_folder, f\"cartpole_results_{timestamp}.json\")\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "logger.info(f\"Results saved to {output_file}\")\n",
    "\n",
    "# Print summary\n",
    "average_reward = sum(r['total_reward'] for r in results) / len(results)\n",
    "average_length = sum(r['length'] for r in results) / len(results)\n",
    "logger.info(f\"Test completed. Average reward: {average_reward:.2f}, Average length: {average_length:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "\n",
    "1. Set up an LLM-based wrapper for the CartPole environment\n",
    "2. Train a reinforcement learning agent using this environment\n",
    "3. Test the trained agent and collect performance metrics\n",
    "\n",
    "This approach combines the decision-making capabilities of a large language model with the learning process of reinforcement learning, potentially leading to interesting and novel solutions to the CartPole problem.\n",
    "\n",
    "Feel free to experiment with different prompts, training parameters, or even different environments to see how this approach can be applied in various scenarios!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
